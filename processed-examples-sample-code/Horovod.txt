Use Case: Horovod is a distributed deep learning training framework for TensorFlow, Keras, PyTorch, and MXNet. It allows training deep learning models across multiple GPUs on a single node or multiple nodes.
  
Code details and examples:
- Sample input file (Python script using Horovod to train a model):
```python
import tensorflow as tf
import horovod.tensorflow as hvd

# Initialize Horovod
hvd.init()

# Pin GPU to be used to process local rank (one GPU per process)
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], 'GPU')

# Build model
model = tf.keras.applications.ResNet50()

# Horovod: adjust learning rate based on number of GPUs
opt = tf.optimizers.Adam(0.001 * hvd.size())
opt = hvd.DistributedOptimizer(opt)

model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Horovod: adjust number of steps when training
model.fit(train_dataset, steps_per_epoch=500 // hvd.size(), epochs=10)
```

- Command to run the script using Horovod:
```bash
horovodrun -np 4 python train_model.py
```

In this example, we are using Horovod to distribute the training of a ResNet50 model across 4 GPUs.

Note: Horovod needs to be installed and properly configured before using it in the script.